{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUh0mgtZVhpxob9I6/ilTx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mannshah2732/datascience_assignment/blob/main/Feature_Engineering_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?\n",
        "\n",
        "A parameter is a value that the model learns from the data (or a setting you pass to a function).\n",
        "\n",
        "In ML models: weights and biases in a neural network are parameters the model learns.\n",
        "\n",
        "In functions: a parameter is an input variable defined in the function.\n",
        "\n",
        "def greet(name):  # 'name' is a parameter\n",
        "\n",
        "    print(f\"Hello, {name}!\")"
      ],
      "metadata": {
        "id": "sLf8kyuc8Ezo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is correlation? What does negative correlation mean?\n",
        "\n",
        "Correlation measures the relationship between two variables — specifically, how they move together.\n",
        "\n",
        "The strength of correlation is measured by the correlation coefficient (often r) which ranges from -1 to 1.\n",
        "\n",
        "+1    Perfect Positive correlation\n",
        "\n",
        "0     No correlation\n",
        "\n",
        "-1    Perfect Negative correlation\n",
        "\n",
        "A negative correlation means that as one variable increases, the other decreases.\n",
        "\n",
        "For example:\n",
        "\n",
        "As the price of a product increases, demand often decreases."
      ],
      "metadata": {
        "id": "Ftqjnkkk89Qn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "Machine Learning is a field of artificial intelligence (AI) that focuses on building systems that can learn from data and improve over time without being explicitly programmed.\n",
        "\n",
        "Main Components of Machine Learning\n",
        "\n",
        "1.  Data\n",
        "\n",
        "The foundation of any ML system.\n",
        "\n",
        "Includes features (inputs) and sometimes labels (outputs).\n",
        "\n",
        "2. Model\n",
        "\n",
        "A mathematical representation or algorithm that learns from the data.\n",
        "\n",
        "3. Algorithm\n",
        "\n",
        "The process or method used to train the model on data.\n",
        "\n",
        "4. Training\n",
        "\n",
        "The phase where the model learns patterns from the data.\n",
        "\n",
        "5. Testing\n",
        "\n",
        "After training, the model is tested on unseen data to check how well it performs.\n",
        "\n",
        "6. Prediction\n",
        "\n",
        "The trained model is used to make predictions or decisions based on new input data.\n",
        "\n",
        "7. Features\n",
        "\n",
        "Individual variables or inputs used to train the model.\n",
        "\n",
        "8. Targets\n",
        "\n",
        "The output the model is trying to predict or classify."
      ],
      "metadata": {
        "id": "TQaT3d4f9rir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "Loss is a number that shows how far off your model's predictions are from the actual values.\n",
        "\n",
        "The higher the loss, the worse the model is doing.\n",
        "\n",
        "The lower the loss, the better the predictions.\n",
        "\n",
        "Example\n",
        "\n",
        "Let's say you're predicting house prices :\n",
        "\n",
        "   Actual Price\t     Predicted Price\t       Loss (e.g., squared error)\n",
        "\n",
        "1. $500,000\t       $510,000\t                (10,000)² = 100,000,000\n",
        "\n",
        "2. $300,000\t       $290,000\t                (10,000)² = 100,000,000\n",
        "\n",
        "\n",
        "If your model's average loss is very high, it means it's making large errors and needs improvement.\n",
        "\n",
        "Common Loss Functions\n",
        "\n",
        "Mean Squared Error (MSE)\n",
        "Mean Absolute Error (MAE)\n",
        "\n",
        "* Loss tells you how wrong your model is.\n",
        "\n",
        "* A low loss = good model, a high loss = bad model.\n",
        "\n",
        "* It's the main thing we try to minimize during trainin"
      ],
      "metadata": {
        "id": "q8KSmj9a-i7R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are continuous and categorical variables?\n",
        "\n",
        "Continuous Variables\n",
        "\n",
        "These are numeric values that can take any value within a range — including decimals.\n",
        "\n",
        "Think: Measurements or quantities.\n",
        "\n",
        " Examples:\n",
        "\n",
        "Height (e.g., 5.9 ft)\n",
        "\n",
        "Weight (e.g., 72.5 kg)\n",
        "\n",
        "Temperature (e.g., 36.6°C)\n",
        "\n",
        "Points to be remebered :\n",
        "\n",
        "* Can be infinite or very large in number.\n",
        "\n",
        "* You can perform arithmetic (add, subtract, average, etc.)\n",
        "\n",
        "Categorical Variables\n",
        "\n",
        "These represent categories or groups — often textual (but can be numeric too).\n",
        "\n",
        "Think: Labels or names.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Gender (Male, Female)\n",
        "\n",
        "Car Brand (Toyota, BMW, Ford)\n",
        "\n",
        "Property Type (Apartment, Townhouse, Villa)\n",
        "\n",
        "Points to be remembered :\n",
        "\n",
        "* Can be nominal (no order, like colors) or ordinal (with order, like Low/Medium/High).\n",
        "\n",
        "* You can't do math with them meaningfully (e.g., BMW * Toyota)."
      ],
      "metadata": {
        "id": "EktcTFenAKrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "  1. Label Encoding:\n",
        "\n",
        "  Each category is converted to a unique number.\n",
        "\n",
        "  Red     0\n",
        "\n",
        "  Green   1\n",
        "\n",
        "  Blue    2\n",
        "\n",
        "  * Simple and fast\n",
        "\n",
        "  * Be careful: This introduces an artificial order, which can mislead models that assume order matters (like linear regression).\n",
        "\n",
        "  2. One-Hot Encoding:\n",
        "\n",
        "  Creates a binary (0/1) column for each category.\n",
        "\n",
        "  Color   Red   Green\t  Blue\n",
        "\n",
        "  Red\t    1\t    0\t      0\n",
        "\n",
        "  Blue\t  0\t    0\t      1\n",
        "\n",
        "  Green\t  0\t    1\t      0\n",
        "\n",
        "  *  No false order introduced\n",
        "\n",
        "  * Can cause \"curse of dimensionality\" if there are too many categories.\n",
        "\n",
        "  3. Ordinal Encoding:\n",
        "\n",
        "  Used when categories have a natural order (like Low < Medium < High).\n",
        "\n",
        "  Small   0\n",
        "\n",
        "  Medium  1\n",
        "\n",
        "  Large   2\n",
        "\n",
        "  * Useful when order matters\n",
        "\n",
        "  * Only use this when the order is meaningful"
      ],
      "metadata": {
        "id": "ool_Wpr1BCDA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What do you mean by training and testing a dataset?\n",
        "\n",
        "When building a machine learning model, you split your dataset into two main parts:\n",
        "\n",
        "1.  Training Set\n",
        "\n",
        "* This is the part of the data used to train the model.\n",
        "\n",
        "* The model learns patterns, relationships, and rules from this data.\n",
        "\n",
        "2. Testing Set\n",
        "\n",
        "* This is separate data the model has never seen before.\n",
        "\n",
        "* Used to evaluate how well the model performs on new, unseen data.\n",
        "\n",
        " Why Split the Data?\n",
        "\n",
        "Because if you train and test on the same data:\n",
        "\n",
        "* The model might just memorize the data.\n",
        "\n",
        "* It would look accurate but fail on real-world or future data."
      ],
      "metadata": {
        "id": "spI1UytuEQqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is sklearn.preprocessing?\n",
        "\n",
        "sklearn.preprocessing is a module in scikit-learn that contains tools to prepare (or \"preprocess\") your data before feeding it into a machine learning model.\n",
        "\n",
        "Preprocessing is all about cleaning, transforming, and scaling your data so that models can understand and learn from it effectively.\n",
        "\n",
        "* Common Tasks in sklearn.preprocessing:\n",
        "\n",
        " Standardization\n",
        "\n",
        " Normalization"
      ],
      "metadata": {
        "id": "unG_epXW86Yl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is a Test set?\n",
        "\n",
        "A test set is a portion of your dataset that you use to evaluate your trained machine learning model.\n",
        "\n",
        "It's not used during training — it's like \"new, unseen data\" that helps you check how well your model performs in the real world.\n",
        "\n",
        "Think of it like this:\n",
        "\n",
        "Training set = Study material\n",
        "\n",
        "Test set = Final exam"
      ],
      "metadata": {
        "id": "94mbLCmc9ccE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?\n",
        "\n",
        "\n",
        "1.  Split the Data\n",
        "\n",
        "* We use the train_test_split function from scikit-learn to divide the dataset into training and testing sets.\n",
        "\n",
        "* Example:\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Let's say you have features X and target y\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,                # Features and target\n",
        "    test_size=0.2,       # 20% test, 80% train\n",
        "    random_state=42      # For reproducibility\n",
        "    )\n",
        "\n",
        " 2. Approach a Machine Learning Problem\n",
        "\n",
        "Here's a step-by-step roadmap to solve any ML problem:\n",
        "\n",
        "* Step 1: Understand the Problem\n",
        "\n",
        "  * What are you predicting? (target/label)\n",
        "  * What are the inputs? (features)\n",
        "\n",
        "* Step 2: Collect and Explore the Data\n",
        "\n",
        "  * Load the dataset\n",
        "  * Use .head(), .info(), .describe()\n",
        "\n",
        "* Step 3: Preprocess the Data\n",
        "\n",
        "  * Handle missing values (SimpleImputer)\n",
        "  * Encode categorical variables (LabelEncoder, OneHotEncoder)\n",
        "\n",
        "* Step 4: Split the Data\n",
        "\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "* Step 5: Train the Model\n",
        "\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "\n",
        "    model = LinearRegression()\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "* Step 6: Evaluate the Model\n",
        "\n",
        "    from sklearn.metrics import mean_squared_error, r2_score\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
        "    print(\"R² Score:\", r2_score(y_test, y_pred))\n",
        "\n",
        "* Step 7: Tune and Improve\n",
        "\n",
        "  * Try different algorithms\n",
        "\n",
        "  * Use cross-validation"
      ],
      "metadata": {
        "id": "XyUyrZVj93B3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "EDA (Exploratory Data Analysis) is like being a detective before jumping into model building — it helps you understand your data deeply and make smarter modeling decisions.\n",
        "\n",
        "1. Understand the Structure of the Data\n",
        "\n",
        "* What columns are there?\n",
        "* What are the data types? (numeric, categorical, etc.)\n",
        "* Are there any weird values or outliers?\n",
        "\n",
        "2. Detect Missing or Incorrect Data\n",
        "\n",
        "* Models can break or perform poorly if there are NaNs or corrupt values.\n",
        "* EDA helps you identify and handle them before modeling.\n",
        "* df.isnull().sum()  # shows missing values\n",
        "\n",
        "3. Understand Relationships Between Features and Target\n",
        "\n",
        "* Helps you find which features are important or correlated with the target.\n",
        "\n",
        "4. Choose the Right Preprocessing Steps\n",
        "\n",
        "EDA helps decide:\n",
        "* Which features need scaling?\n",
        "* Which ones need encoding?\n",
        "* Do we need feature engineering?\n",
        "\n",
        "5. Spot Outliers\n",
        "\n",
        "* Outliers can distort model performance."
      ],
      "metadata": {
        "id": "EXjOCYILCpQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is correlation?\n",
        "\n",
        "Correlation refers to a statistical measure that describes the extent to which two variables are related to each other. It indicates whether an increase or decrease in one variable is associated with an increase or decrease in another.\n",
        "\n",
        "The correlation coefficient (often represented by r) quantifies this relationship. It ranges from -1 to 1:\n",
        "\n",
        "* +1 indicates a perfect positive correlation: as one variable increases, the other also increases in exact proportion.\n",
        "\n",
        "* -1 indicates a perfect negative correlation: as one variable increases, the other decreases in exact proportion.\n",
        "\n",
        "* 0 indicates no correlation: changes in one variable do not predict changes in the other."
      ],
      "metadata": {
        "id": "-S6ZkPEqcsYv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What does negative correlation mean?\n",
        "\n",
        "A negative correlation means that as one variable increases, the other variable tends to decrease, and vice versa. In other words, there is an inverse relationship between the two variables.\n",
        "\n",
        "For example:\n",
        "\n",
        "* If you were to examine the relationship between the amount of time spent studying and the number of errors made on a test, you might find a negative correlation: as the time spent studying increases, the number of errors decreases.\n",
        "\n",
        "In terms of the correlation coefficient:\n",
        "\n",
        "* A negative correlation is indicated by a coefficient between -1 and 0."
      ],
      "metadata": {
        "id": "LB2_ytQ2dF5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How can you find correlation between variables in Python?\n",
        "\n",
        "To find the correlation between variables in Python, you can use libraries such as Pandas and NumPy. The most common method is using the .corr() function from Pandas, which calculates the correlation matrix for a DataFrame.\n",
        "\n",
        "Step by Step guide\n",
        "\n",
        "  1. Install and Import the Necessary Libraries:\n",
        "  2. Calculate Correlation Using Pandas"
      ],
      "metadata": {
        "id": "OBXhLqVJdX8g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "Causation refers to a cause-and-effect relationship between two variables. It means that one variable directly influences or causes a change in another variable. If A causes B, it means that changes in A will bring about changes in B.\n",
        "\n",
        "Correlation vs. Causation :\n",
        "\n",
        "* While correlation shows that two variables are related in some way (i.e., they tend to vary together), it does not imply that one variable causes the other to change. Causation, on the other hand, means that one variable is directly responsible for the change in another.\n",
        "\n",
        "Key Differences :\n",
        "\n",
        "  1. Correlation:\n",
        "\n",
        "  * Describes a relationship or pattern between two variables.\n",
        "  * Does not imply that one variable causes the other to change.\n",
        "  * Can be positive, negative, or zero (no relationship).\n",
        "\n",
        "  2. Causation:\n",
        "\n",
        "  * Implies that one variable directly influences or causes a change in the other.\n",
        "  * Shows a cause-and-effect relationship.\n",
        "  * Requires more rigorous testing (such as experiments) to confirm."
      ],
      "metadata": {
        "id": "B1apDuzAdw56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "* An optimizer is an algorithm or method used in machine learning and deep learning to minimize or maximize a function, typically the loss function (also called the objective function).\n",
        "\n",
        "* The goal of the optimizer is to find the best set of parameters (like weights in a neural network) that minimize the error (loss) or maximize the performance (accuracy) of the model.\n",
        "\n",
        "  # Types of Optimizers:\n",
        "\n",
        "  1. Gradient Descent (GD) :\n",
        "\n",
        "  * Gradient Descent is the most basic and widely used optimization algorithm. It updates the model’s parameters by moving them in the direction of the negative gradient (i.e., the steepest descent) of the loss function with respect to the parameters.\n",
        "\n",
        "  * Formula: 𝜃=𝜃−𝜂⋅∇𝜃𝐽(𝜃)\n",
        "\n",
        "  * Where:\n",
        "  𝜃 = Parameters (weights) of the model\n",
        "\n",
        "  𝜂 = Learning rate (how much to adjust the parameters)\n",
        "\n",
        "  ∇𝜃𝐽(𝜃) = Gradient of the loss function J(θ) with respect to parameters\n",
        "\n",
        "  * Example :\n",
        "\n",
        "  * If you are training a linear regression model, gradient descent will adjust the coefficients of the model (weights) to reduce the error (difference between the predicted and actual values).\n",
        "  \n",
        "  2. Stochastic Gradient Descent (SGD) :\n",
        "  \n",
        "  * Stochastic Gradient Descent is a variation of gradient descent where the gradient is calculated for each individual data point rather than the entire dataset.\n",
        "\n",
        "  * Example:\n",
        "  \n",
        "  * In deep learning, for training large neural networks, SGD updates the weights after every training sample, making it computationally more efficient compared to batch gradient descent for very large datasets."
      ],
      "metadata": {
        "id": "FqvwAmtyeZMC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What is sklearn.linear_model ?\n",
        "\n",
        "* sklearn.linear_model is a module within Scikit-learn, a popular machine learning library in Python.\n",
        "\n",
        "* This module contains various classes and functions that implement linear models for regression and classification tasks.\n",
        "\n",
        "* Linear models are used when the relationship between the dependent (target) variable and one or more independent (predictor) variables is assumed to be linear.\n",
        "\n",
        "Here are some of the most commonly used linear models in this module:\n",
        "\n",
        "1. LinearRegression\n",
        "2. Ridge\n",
        "3. Lasso\n",
        "4. ElasticNet\n",
        "5. LogisticRegression"
      ],
      "metadata": {
        "id": "yxJ2Ozcngxqj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What does model.fit() do? What arguments must be given?\n",
        "\n",
        "* In machine learning, the fit() method is used to train a model.\n",
        "* When you call model.fit(X, y), it tells the model to learn the patterns from the provided training data (X and y). Specifically:\n",
        "\n",
        "X is the feature matrix, which contains the input data (independent variables).\n",
        "\n",
        "y is the target vector, which contains the output or labels (dependent variable).\n",
        "\n",
        "# What Arguments Must Be Given?\n",
        "\n",
        "* The two main arguments that must be given to model.fit() are:\n",
        "\n",
        "1. X (Features or Input Data):\n",
        "\n",
        "  * This is the matrix or array of input data for training.\n",
        "\n",
        "  * It should be in the form of a 2D array (matrix) where each row represents a sample (data point) and each column represents a feature (predictor variable).\n",
        "\n",
        "  * Shape: (n_samples, n_features).\n",
        "\n",
        "2. y (Target or Labels):\n",
        "\n",
        "  * This is the vector or array containing the target values (the output that the model should predict).\n",
        "\n",
        "  * For regression tasks, y contains continuous values.\n",
        "\n",
        "  * For classification tasks, y contains categorical labels.\n",
        "\n",
        "  * Shape: (n_samples,).\n",
        "\n"
      ],
      "metadata": {
        "id": "xEBHJ0-VhTzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What does model.predict() do? What arguments must be given?\n",
        "\n",
        "* The model.predict() method is used to make predictions using a trained machine learning model.\n",
        "* After the model has been trained with the fit() method on the training data, the predict() method applies the learned patterns to new, unseen data to predict the output (target values).\n",
        "\n",
        "\n",
        "The primary argument that must be provided to the predict() method is:\n",
        "\n",
        "1. X (Features or Input Data):\n",
        "\n",
        "  * This is the matrix or array of input data for which you want to make predictions.\n",
        "\n",
        "  * It should be in the same format as the data used to train the model (i.e., the same number of features).\n",
        "\n",
        "  * Shape: (n_samples, n_features), where:\n",
        "    * n_samples: The number of data points you want to predict.\n",
        "    * n_features: The number of features each data point has.\n",
        "\n"
      ],
      "metadata": {
        "id": "FUbo2Oo5idui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What are continuous and categorical variables?\n",
        "\n",
        "  1. Continuous Variables :\n",
        "\n",
        "  * Continuous variables are variables that can take an infinite number of values within a given range.\n",
        "\n",
        "  * These values are typically numerical and can represent measurable quantities.\n",
        "\n",
        "  * Continuous variables can be discrete at times (if measurements are rounded or limited), but in theory, they can take any value, including decimals or fractions.\n",
        "\n",
        "  2. Categorical Variables :\n",
        "\n",
        "  * Categorical variables are variables that take on a limited and fixed number of categories or values.\n",
        "  * These variables represent types or categories of data and do not have a meaningful numeric order or scale.\n",
        "  * Categorical variables can be either nominal or ordinal."
      ],
      "metadata": {
        "id": "LGc48fxrjCFA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "* Feature scaling is a preprocessing technique in machine learning used to standardize or normalize the range of independent variables (features).\n",
        "\n",
        "* It ensures that all features contribute equally to the model’s performance by bringing them to a similar scale.\n",
        "\n",
        "* Feature Scaling Importance\n",
        "\n",
        "  * Many machine learning algorithms rely on distance-based calculations (like gradient descent, KNN, SVM, or clustering). If the features have vastly different scales, the algorithm may become biased toward features with larger values, leading to:\n",
        "\n",
        "    * Poor model performance\n",
        "\n",
        "    * Longer training times\n",
        "\n",
        "    * Unstable convergence\n",
        "    \n",
        "* How Feature Scaling Helps :\n",
        "\n",
        "    * Makes training faster and more stable.\n",
        "\n",
        "    * Prevents features with larger scales from dominating the model.\n",
        "\n",
        "    * Improves performance of distance-based algorithms (e.g., K-Means, KNN, SVM).\n",
        "\n",
        "    * Helps gradient descent converge more efficiently.\n",
        "\n",
        "    * Essential for algorithms that are sensitive to magnitude and unit differences."
      ],
      "metadata": {
        "id": "Xmxd-Mz6OYxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. How do we perform scaling in Python?\n",
        "\n",
        "Feature scaling in Python is typically done using Scikit-learn’s preprocessing module. Below are the most common scaling methods and how to implement them.\n",
        "\n",
        "1.  Standardization (Z-score Scaling) :\n",
        "Scales features to have mean = 0 and standard deviation = 1.\n",
        "\n",
        "2. Min-Max Scaling (Normalization)\n",
        "Scales data to a fixed range, usually [0, 1].\n",
        "\n",
        "3. Robust Scaling\n",
        "Uses median and IQR. Great for data with outliers.\n",
        "\n",
        "4. MaxAbs Scaling\n",
        "Scales features to range [-1, 1] based on the maximum absolute value. Good for sparse data.\n",
        "\n",
        "*  General Steps:\n",
        "\n",
        "  1. Import the scaler you want to use.\n",
        "\n",
        "  2. Create an instance of the scaler.\n",
        "\n",
        "  3. Fit and transform your features using .fit_transform().\n",
        "\n"
      ],
      "metadata": {
        "id": "vlf8ji07Pkg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is sklearn.preprocessing?\n",
        "\n",
        "* Sklearn.preprocessing is a module in Scikit-learn that provides a wide range of tools for data preprocessing and transformation.\n",
        "\n",
        "* These tools help prepare your dataset for machine learning models by scaling, encoding, normalizing, or transforming the data into a suitable format.\n",
        "\n",
        "  * Why we use sklearn.preprocessing\n",
        "\n",
        "  Many machine learning algorithms require:\n",
        "\n",
        "  * Features to be on the same scale (e.g., KNN, SVM, Gradient Descent).\n",
        "\n",
        "  * Non-numeric data to be converted into numerical form (e.g., categorical to one-hot).\n",
        "\n",
        "  * Input features to follow a certain distribution (e.g., Gaussian)."
      ],
      "metadata": {
        "id": "42UVnzOjS-jq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "* To train and evaluate a machine learning model properly, we split our dataset into training and testing sets. This ensures the model learns from one portion (training data) and is evaluated on unseen data (test data).\n",
        "\n",
        "*  Use train_test_split() from Scikit-learn\n",
        "  The most common method is:\n",
        "\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "* Basic Syntax:\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "* How split works\n",
        "\n",
        "  If you have 1000 samples:\n",
        "\n",
        "  test_size=0.2 → 800 samples go to training, 200 to testing.\n",
        "\n"
      ],
      "metadata": {
        "id": "VUwbvlPed32V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Explain data encoding?\n",
        "\n",
        "* Data encoding is the process of converting categorical data (non-numeric values like labels, names, or classes) into a numerical format that can be used by machine learning algorithms.\n",
        "\n",
        "* Why is Enocding necessary\n",
        "\n",
        "  Most machine learning models (like regression, SVM, neural networks) require numerical input. Since categorical data like \"Male\", \"Female\", \"Red\", \"Blue\" cannot be directly used, they must be encoded into numbers.\n",
        "\n",
        "* Types of Data Encoding\n",
        "\n",
        "  1. Label Encoding\n",
        "  2. One-Hot Encoding\n",
        "  3. Ordinal Encoding\n",
        "  4. Binary Encoding, Hash Encoding (Advanced)"
      ],
      "metadata": {
        "id": "WjHDplFMebek"
      }
    }
  ]
}